---
title: getting started with bayesian inference
author: ''
date: '2020-06-12'
slug: getting-started-with-bayesian-inference
categories:
  - statistics
tags:
  - statistics
  - R
  - bayesian
description: ''
---


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



```{r}
shotData<- c(1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
             1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
             0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0)
```


```{r, include=FALSE}



plot.beta <- function(PS = 1, PF = 1, k = 0, n = 0, null = NULL, CI = NULL, ymax = "auto", main = NULL) {
        
        x = seq(.001, .999, .001) ##Set up for creating the distributions
        y1 = dbeta(x, PS, PF) # data for prior curve
        y3 = dbeta(x, PS + k, PF + n - k) # data for posterior curve
        y2 = dbeta(x, 1 + k, 1 + n - k) # data for likelihood curve, plotted as the posterior from a beta(1,1)
        
        if(is.numeric(ymax) == T){ ##you can specify the y-axis maximum
                y.max = ymax
        }        
        else(
                y.max = 1.25 * max(y1,y2,y3,1.6) ##or you can let it auto-select
        )
        
        if(is.character(main) == T){
                Title = main
        }
        else(
                Title = "Prior-to-Posterior Transformation with Binomial Data"
        )
        
        
        plot(x, y1, xlim=c(0,1), ylim=c(0, y.max), type = "l", ylab= "Density", lty = 2,
             xlab= "Probability of success", las=1, main= Title,lwd=3,
             cex.lab=1.5, cex.main=1.5, col = "skyblue", axes=FALSE)
        
        axis(1, at = seq(0,1,.2)) #adds custom x axis
        axis(2, las=1) # custom y axis
        
        
        
        if(n != 0){
                #if there is new data, plot likelihood and posterior
                lines(x, y2, type = "l", col = "darkorange", lwd = 2, lty = 3)
                lines(x, y3, type = "l", col = "darkorchid1", lwd = 5)
                legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("skyblue", "darkorchid1", "darkorange"), 
                       lty = c(2,1,3), lwd = c(3,5,2), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
                
                ## adds null points on prior and posterior curve if null is specified and there is new data
                if(is.numeric(null) == T){
                        ## Adds points on the distributions at the null value if there is one and if there is new data
                        points(null, dbeta(null, PS, PF), pch = 21, bg = "blue", cex = 1.5)
                        points(null, dbeta(null, PS + k, PF + n - k), pch = 21, bg = "darkorchid", cex = 1.5)
                        abline(v=null, lty = 5, lwd = 1, col = "grey73")
                        ##lines(c(null,null),c(0,1.11*max(y1,y3,1.6))) other option for null line
                }
        }
        
        ##Specified CI% but no null? Calc and report only CI
        if(is.numeric(CI) == T && is.numeric(null) == F){
                CI.low <- qbeta((1-CI)/2, PS + k, PF + n - k)
                CI.high <- qbeta(1-(1-CI)/2, PS + k, PF + n - k)
                
                SEQlow<-seq(0, CI.low, .001)
                SEQhigh <- seq(CI.high, 1, .001)
                ##Adds shaded area for x% Posterior CIs
                cord.x <- c(0, SEQlow, CI.low) ##set up for shading
                cord.y <- c(0,dbeta(SEQlow,PS + k, PF + n - k),0) ##set up for shading
                polygon(cord.x,cord.y,col='orchid', lty= 3) ##shade left tail
                cord.xx <- c(CI.high, SEQhigh,1) 
                cord.yy <- c(0,dbeta(SEQhigh,PS + k, PF + n - k), 0)
                polygon(cord.xx,cord.yy,col='orchid', lty=3) ##shade right tail
                
                return( list( "Posterior CI lower" = round(CI.low,3), "Posterior CI upper" = round(CI.high,3)))
        }
        
        ##Specified null but not CI%? Calculate and report BF only 
        if(is.numeric(null) == T && is.numeric(CI) == F){
                null.H0 <- dbeta(null, PS, PF)
                null.H1 <- dbeta(null, PS + k, PF + n - k)
                CI.low <- qbeta((1-CI)/2, PS + k, PF + n - k)
                CI.high <- qbeta(1-(1-CI)/2, PS + k, PF + n - k)
                return( list("BF01 (in favor of H0)" = round(null.H1/null.H0,3), "BF10 (in favor of H1)" = round(null.H0/null.H1,3)
                ))
        }
        
        ##Specified both null and CI%? Calculate and report both
        if(is.numeric(null) == T && is.numeric(CI) == T){
                null.H0 <- dbeta(null, PS, PF)
                null.H1 <- dbeta(null, PS + k, PF + n - k)
                CI.low <- qbeta((1-CI)/2, PS + k, PF + n - k)
                CI.high <- qbeta(1-(1-CI)/2, PS + k, PF + n - k)
                
                SEQlow<-seq(0, CI.low, .001)
                SEQhigh <- seq(CI.high, 1, .001)
                ##Adds shaded area for x% Posterior CIs
                cord.x <- c(0, SEQlow, CI.low) ##set up for shading
                cord.y <- c(0,dbeta(SEQlow,PS + k, PF + n - k),0) ##set up for shading
                polygon(cord.x,cord.y,col='orchid', lty= 3) ##shade left tail
                cord.xx <- c(CI.high, SEQhigh,1) 
                cord.yy <- c(0,dbeta(SEQhigh,PS + k, PF + n - k), 0)
                polygon(cord.xx,cord.yy,col='orchid', lty=3) ##shade right tail
                
                return( list("BF01 (in favor of H0)" = round(null.H1/null.H0,3), "BF10 (in favor of H1)" = round(null.H0/null.H1,3),
                             "Posterior CI lower" = round(CI.low,3), "Posterior CI upper" = round(CI.high,3)))
        }
        
}


```


Last week, we spoke about hypothesis testing from a frequentist perspective.
Today, we're going to explore another way of statistics called bayesian statistics. Bayesian statistics simply makes inferences based on bayes theorem.

Assumes prior knowledge on conditional, marginal and random variable concepts.


Mathematical definition of bayes: $P(A/B) = \dfrac {P(B/A)P(A)}{P(B)}$


$posterior = \dfrac {prior * likelihood} {marginal \hspace{0.2cm} likelihood}$


There are three main ingredients required for bayesian analysis:

1) Prior
2) Likelihood
3) Posterior

Let's start with an example:


Let's say you have a headache and sore throat, and you know that some people in the office you work at have also had the flu. Does this mean you have the flu?


Wanting to gain a little more information you roll over, grab your phone
and search Google. You find a reputable article that says that only 5% of the
population will get the flu in a given year. Ok. So, the probability of having
the flu, in general, is only 5%.

$P(flu/symptoms) =\dfrac {P(symptoms/flu)P(flu)}{P(symptoms)}$

$P(flu)$ = 0.05
$P(symptoms/flu)$ = 0.9



### prior

The first ingredient is the background knowledge on the parameters of the model being tested. This refers to the knowledge available _before_ seeing the data. 

Let's assume you don't know anything about the prevalence of flu. So you'll probably pick a prior where all probabilities are equally likely.  It will look something like this

<plot the prior plot>

```{r}
plot.beta(1,1,ymax=3.2,main="Uniform Prior, Beta(1,1)")
```


Wanting to gain a little more information you roll over, grab your phone
and search Google. You find a reputable article that says that only 5% of the
population will get the flu in a given year. But another article says that the prevalence could be a little higher. In this case a distribution of our prior belief might be more appropriate to incorporate our uncertainity. This distribution is known as the __prior distribution__.

<plot another prior dist>


```{r}
plot.beta(4,9,ymax=3.2,main="Informed Prior, Beta(4,9)")
```



### Likelihood


The Likelihood Principle states that the likelihood function contains all of the information relevant to the evaluation of statistical evidence. This makes likelihood the workhorse of bayesian inference. 






