<!DOCTYPE html>
<html
  class=""
  lang="en-us"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  <head>
    <meta charset="utf-8" />

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="description" content="" />
<meta name="HandheldFriendly" content="True" />
<meta name="MobileOptimized" content="320" />
<meta name="viewport" content="width=device-width, initial-scale=1" />


<meta name="keywords" content="statistics,
R,
bayesian,
">


<meta property="og:type" content="article" />
<meta property="og:description" content="" />
<meta property="og:title" content="getting started with bayesian inference" />
<meta property="og:site_name" content="Welcome" />
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="/post/getting-started-with-bayesian-inference/" />
<meta property="og:locale" content="en-us" />
<meta property="article:published_time" content="2020-06-27
" /> <meta property="article:modified_time" content="2020-06-27
" />


<meta property="article:tag" content="statistics" />
<meta property="article:tag" content="R" />
<meta property="article:tag" content="bayesian" />




<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@siegerts" />
<meta name="twitter:creator" content="@siegerts" />
<meta
  name="twitter:title"
  content="getting started with bayesian inference | Welcome"
/>
<meta
  name="twitter:description"
  content="In my previous post, we spoke about hypothesis testing from a frequentist perspective. If you took a stats class, that is the method you probably learn. There is another, arguably better method to conduct a hypothesis testing and that using bayesian inference.
My goal in this blogpost is to break down the basics of bayesian inference.
This blogpost assumes prior knowledge on conditional, marginal and random variable concepts.|"
/>
<meta name="twitter:image:src" content="" />
<meta name="twitter:domain" content="/post/getting-started-with-bayesian-inference/" />



    <title>getting started with bayesian inference</title>
    <link rel="canonical" href="/post/getting-started-with-bayesian-inference/" />


    <link
  rel="stylesheet"
  href="https://unpkg.com/tachyons@4.11.1/css/tachyons.min.css"
/>

<link rel="stylesheet" href="/css/style.css" />

<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/highlightjs@9.12.0/styles/github-gist.css"
/>


<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	if (window.sessionStorage) {
		var GA_SESSION_STORAGE_KEY = 'ga:clientId';
		ga('create', 'UA-XXXXXXXXX-X', {
	    'storage': 'none',
	    'clientId': sessionStorage.getItem(GA_SESSION_STORAGE_KEY)
	   });
	   ga(function(tracker) {
	    sessionStorage.setItem(GA_SESSION_STORAGE_KEY, tracker.get('clientId'));
	   });
   }
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
  </head>


<body
  lang="en-us"
  class="sans-serif w-90 w-80-m w-60-ns center mv2 mv5-ns"
  itemscope
  itemtype="http://schema.org/Article"
>
  
  <span class="b">/ </span>
  <a href="/" class="b bb bw1 pb1 no-underline black">Welcome</a>
  <span class="b"> / </span>
  <a href="/post" class="b bb bw1 pb1 no-underline black">blog</a>

  <section id="main" class="mt5">
    <h1 itemprop="name" id="title">getting started with bayesian inference</h1>
    <span class="f6 gray">June 27, 2020</span>

      <article itemprop="articleBody" id="content" class="w-90 lh-copy">
        


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<p><a href="https://apoorvasrinivasanblog.com/post/hypothesis-testing/">In my previous post</a>, we spoke about hypothesis testing from a frequentist perspective. If you took a stats class, that is the method you probably learn. There is another, arguably better method to conduct a hypothesis testing and that using bayesian inference.</p>
<p>My goal in this blogpost is to break down the basics of bayesian inference.</p>
<p>This blogpost assumes prior knowledge on conditional, marginal and random variable concepts.</p>
<div id="why-go-bayes" class="section level3">
<h3>why go bayes?</h3>
<p>there‚Äôs tons of material on this by poeple</p>
<div id="bayes-theorem" class="section level4">
<h4>bayes theorem</h4>
<p>Before getting into the inference, let‚Äôs take a moment to talk about bayes theorem. As one of the most popular formula in statistics, it needs no introduction but it may be helpful to state it here again.</p>
<p>Mathematical definition of bayes: <span class="math inline">\(P(A/B) = \dfrac {P(B/A)P(A)}{P(B)}\)</span></p>
<p>There are three main ingredients to bayesian hypothesis:
1) Prior
2) Likelihood
3) Posterior</p>
<p>They can be put together into the bayes theorem as shown:</p>
<p><span class="math inline">\(posterior = \dfrac {prior * likelihood} {marginal \hspace{0.2cm} likelihood}\)</span></p>
<p>Let‚Äôs start with an example:</p>
<p>Let‚Äôs say you have a headache and sore throat, and you know that some people in the office you work at have also had the flu. Does this mean you have the flu? Let‚Äôs find out if bayesian inference can help us figure it out.</p>
<p>Putting our problem in mathematical terms, it looks like this:</p>
<p><span class="math inline">\(P(flu| symptoms) = \dfrac {P(symptoms| flu) * P(flu)} {P(symptoms)}\)</span></p>
</div>
</div>
<div id="prior" class="section level1">
<h1>prior</h1>
<p>The first ingredient is the background knowledge on the parameters of the model being tested. This refers to the knowledge available <em>before</em> seeing the data.</p>
<p>Let‚Äôs assume you don‚Äôt know anything about the prevalence of flu. So you‚Äôll probably pick a prior where all probabilities are equally likely. It will look something like this</p>
<p><plot the prior plot></p>
<pre class="r"><code>plot.beta(1,1,ymax=3.2,main=&quot;Uniform Prior, Beta(1,1)&quot;)</code></pre>
<p><img src="/post/2020-06-12-an-attempt-at-bayesian-hypothesis-testing_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Wanting to gain a little more information you roll over, grab your phone
and search Google. You find an article that says that only 12% of the
population will get the flu in a given year. But another article says that the prevalence could be a little higher. In this case a distribution of our prior belief might be more appropriate to incorporate our uncertainity. This distribution is known as the <strong>prior distribution</strong>.</p>
<p><plot another prior dist></p>
<pre class="r"><code>plot.beta(12,88,ymax=15,main=&quot;Informed Prior, Beta(12, 88)&quot;)</code></pre>
<p><img src="/post/2020-06-12-an-attempt-at-bayesian-hypothesis-testing_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>üìù
In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the binomial distribution. Computing a posterior using a conjugate prior is very convenient, because you can avoid expensive numerical computation. You can learn more about conjugate priors here(add link)</p>
</div>
<div id="likelihood" class="section level1">
<h1>likelihood</h1>
<p>The second ingredient is likelihood, this is where we extract information from our data. Likelihood function is the one that does all the work in bayesian inference. It is also one of the trickier concepts to understand in one shot.</p>
<p>Lets say you go out and collect data from 100 of your colleagues and 30 of them are showing symptoms. The likelihood curve wil look like this:</p>
<pre class="r"><code>x = seq(.001, .999, .001) ##Set up for creating the distributions
y2 = dbeta(x, 1 + 30, 1 + 70) # data for likelihood curve, plotted as the posterior from a beta(1,1)

plot(x, y2, xlim=c(0,1), ylim=c(0, 1.25 * max(y2,1.6)), type = &quot;l&quot;, ylab= &quot;Density&quot;, lty = 3,
     xlab= &quot;Probability of success&quot;, las=1, main=&quot;Likelihood Curve for Flu Data&quot;,lwd=2,
     cex.lab=1.5, cex.main=1.5, col = &quot;cyan3&quot;, axes=FALSE)
axis(1, at = seq(0,1,.2)) #adds custom x axis
axis(2, las=1) # custom y axis</code></pre>
<p><img src="/post/2020-06-12-an-attempt-at-bayesian-hypothesis-testing_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The Likelihood Principle states that the likelihood function contains all of the information relevant to the evaluation of statistical evidence. In other words, facets of the data that do not factor into the likelihood function are irrelevant to the evaluation of the strength of the statistical evidence. Likelihood distribution is the curve that best fits the data we‚Äôre given.</p>
</div>
<div id="is-likelihood-a-probability" class="section level1">
<h1>is likelihood a probability?</h1>
<p>Likelihood is not a probability, but it‚Äôs proportional to it.</p>
<p>The likelihood of a hypothesis (H) given some data (D) is proportional to the probability of obtaining D given that H is true, multiplied by an arbitrary positive constant (K).</p>
<p>Since a likelihood isn‚Äôt actually a probability it doesn‚Äôt obey various rules of probability. For example, likelihood need not sum to 1.</p>
<p>A critical difference between probability and likelihood is in the interpretation of what is fixed and what can vary. The distinction is subtle, so pay close attention. For the conditional probability, the hypotheses is fixed and the data vary. For likelihood, the data are a given and the hypotheses vary.</p>
</div>
<div id="posterior" class="section level1">
<h1>posterior</h1>
<p>Now for the easiest part. Getting the posterior distribution. This is the distribution representing our belief about the parameter values after we have calculated everything on the right hand side taking the observed data into account.</p>
<p><span class="math inline">\(Posterior \propto Prior * Likelihood\)</span></p>
<div id="why-did-we-ignore-the-demonimator" class="section level3">
<h3>why did we ignore the demonimator?</h3>
<p>Well, apart from being the marginal distribution of the data it doesn‚Äôt really have a fancy name, although it‚Äôs sometimes referred to as the evidence. Remember, we‚Äôre only interested in the parameter values but P(data) doesn‚Äôt have any reference to them. In fact, P(data) doesn‚Äôt even evaluate to a distribution. It‚Äôs just a number. we‚Äôve already observed the data so we can calculate P(data). In general, it turns out that calculating P(data) is very hard and so many methods exist to calculate it.</p>
<p>The reason why P(data) is useful is because the number that comes out is a normalising constant. One of the necessary conditions for a probability distribution is that the sum of all possible outcomes of an event is equal to 1. The normalising constant makes sure that the resulting posterior distribution is a true probability distribution by ensuring that the sum of the distribution (I should really say integral because it‚Äôs usually a continuous distribution) is equal to 1.</p>
<p>Coming back to our example, so far we have two distributions: prior and likelihood. We can get the posterior by multiplying the two. The resulting distribution is shown below</p>
<pre class="r"><code>#Initial Priors and final Posteriors after all rounds at once

#par(mfrow=c(1,2))
plot.beta(1,1,30,100,ymax=10,main=&quot;Beta(1,1) to Beta(59,43)&quot;)</code></pre>
<p><img src="/post/2020-06-12-an-attempt-at-bayesian-hypothesis-testing_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>With the uniform prior, the posterior falls right on top of the likelihood function. In the plot above, the likelihood is nearly invisible because the posterior sits right on top of it. When the prior has only 1 or 2 data points worth of information, it has essentially no impact on the posterior shape</p>
<pre class="r"><code>plot.beta(12,88,30,100,ymax=15,main=&quot;Beta(12,88) to Beta(30,70)&quot;)</code></pre>
<p><img src="/post/2020-06-12-an-attempt-at-bayesian-hypothesis-testing_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The second plot above shows how the posterior splits the difference between the likelihood and the informed prior .My takeaway is that the the probability of having the flu based on the data you‚Äôve collected is about 20% which is slightly higher than the prior.</p>
</div>
</div>

      </article>

      
      <span class="f6 gray mv3" title="Lastmod: June 27, 2020. Published at: 2020-06-27.">
        
      </span>

      

  </section>

  <footer>
    <div>
      <p class="f6 gray mt6 lh-copy">
        ¬© 2016-19 <a href='https://github.com/siegerts/hugo-theme-basic'>Hugo Theme Basic</a>. Made by <a href='https://twitter.com/siegerts'>@siegerts</a>.
      </p>
    </div>
  </footer>
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/highlight.min.js"></script>

<script>
  hljs.initHighlightingOnLoad();
</script>



  </body>
</html>
